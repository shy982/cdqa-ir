{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d450d4-a778-4e79-aee4-d061c102d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de2cd29-c1f6-4a98-b843-c2fc4961c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = OpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\")) # uses legacy da-vinci-003 model\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a1864e-1407-419a-a5a6-9643a32c4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return word_tokenize(text.lower())\n",
    "queries = {}\n",
    "\n",
    "idx = None\n",
    "with open(\"../data/cisi/CISI.QRY\") as f:\n",
    "    for query in f.read().split(\".I\"):\n",
    "        for i, line in enumerate(query.split(\"\\n\")):\n",
    "            if not line:\n",
    "                continue\n",
    "            elif i == 0:\n",
    "                idx = int(line)\n",
    "                queries[idx] = \"\"\n",
    "            elif not line.startswith(\".\"):\n",
    "                queries[idx] += \" \"+line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8944e68a-04f8-48f6-b74f-795a5aea06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b71d4aa-fbbf-481b-8070-136f6acaf3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting ChatGPT Responses: 100%|██████████████████████████████████| 112/112 [06:35<00:00,  3.53s/it]\n"
     ]
    }
   ],
   "source": [
    "#### API CALL WARNING ####\n",
    "\n",
    "answers_openai = {}\n",
    "for idx, qry in tqdm(queries.items(), desc = 'Collecting ChatGPT Responses'):\n",
    "\n",
    "    ### Uncomment to use text-davinci-003 Model ###\n",
    "    \n",
    "    # answers_openai[idx] = llm.invoke(qry)\n",
    "    # model = 'text-davinci-003'\n",
    "\n",
    "    ### Uncomment to use gpt-3.5-turbo-instruct Model ###\n",
    "    answer = client.completions.create(\n",
    "                              model=\"gpt-3.5-turbo-instruct\",\n",
    "                              prompt = \"Answer this query or Explain the topic in about 150 words:\" + qry,\n",
    "                              max_tokens = 200\n",
    "                          )\n",
    "    answers_openai[idx] = answer.choices[0].text\n",
    "    model = 'gpt-3.5-turbo-instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed55916-afd8-40d3-92af-fbf2c9c272b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nIn order to retrieve pertinent data automatically in response to information requests, it is crucial to have a well-organized and structured database. This database should contain all the relevant information in a standardized format, making it easier for a computer program or algorithm to search and retrieve specific data. The inclusion of metadata, such as keywords and tags, can also aid in the automatic retrieval of relevant data.\\n\\nAnother important step is to develop a robust search algorithm that is able to understand and interpret the user's information request. This involves training the algorithm using machine learning techniques, so it can accurately understand and interpret natural language queries.\\n\\nTo further enhance the automatic retrieval of pertinent data, it is also beneficial to have a system of user feedback. This allows the algorithm to learn and improve its understanding based on past user searches and selections.\\n\\nLastly, integration with advanced data mining techniques and artificial intelligence can also greatly improve the efficiency and accuracy of automatic data retrieval. These techniques can help identify patterns and relationships within the data, making it\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_openai[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b416fc97-502a-458f-9e35-5404ceae78f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers saved to ./backups/answers_openai_gpt-3.5-turbo-instruct.pkl\n"
     ]
    }
   ],
   "source": [
    "#### DUMP OVERWRITE WARNING ####\n",
    "\n",
    "openai_answers_file_path = './backups/answers_openai_'+model+'.pkl'\n",
    "\n",
    "with open(openai_answers_file_path, 'wb') as file:\n",
    "    pickle.dump(answers_openai, file)\n",
    "\n",
    "print(f\"Answers saved to {openai_answers_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53d7d888-6520-4f05-a471-693db3de55ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document embeddings loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "docs_file_path = './backups/openai_embeddings/doc_embeddings.pkl'\n",
    "\n",
    "with open(docs_file_path, 'rb') as file:\n",
    "    docs = pickle.load(file)\n",
    "\n",
    "print(\"Document embeddings loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6126b150-171e-47fb-834f-2e2bcb8720bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.W\\nThis report is an analysis of 6300 acts of use\\nin 104 technical libraries in the United Kingdom.\\nLibrary use is only one aspect of the wider pattern of\\ninformation use.  Information transfer in libraries is\\nrestricted to the use of documents.  It takes no\\naccount of documents used outside the library, still\\nless of information transferred orally from person\\nto person.  The library acts as a channel in only a\\nproportion of the situations in which information is\\ntransferred.\\nTaking technical information transfer as a whole,\\nthere is no doubt that this proportion is not the\\nmajor one.  There are users of technical information -\\nparticularly in technology rather than science -\\nwho visit libraries rarely if at all, relying on desk\\ncollections of handbooks, current periodicals and personal\\ncontact with their colleagues and with people in other\\norganizations.  Even regular library users also receive\\ninformation in other ways.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30166860-696f-4f6d-8a80-80607694bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_set = {}\n",
    "with open(os.path.join(\"../data/cisi/\", 'CISI.REL')) as f:\n",
    "    for l in f.readlines():\n",
    "        qry_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0]\n",
    "        doc_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1]\n",
    "\n",
    "        if qry_id in rel_set:\n",
    "            rel_set[qry_id].append(doc_id)\n",
    "        else:\n",
    "            rel_set[qry_id] = []\n",
    "            rel_set[qry_id].append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed728ff-891a-4d21-91db-f213734a5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_answers_file_path = './backups/answers_openai_'+model+'.pkl'\n",
    "with open(openai_answers_file_path, 'rb') as file:\n",
    "    answers_openai = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e3d677c-1ac9-49d4-914c-22deb69b3720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing scores:: 100%|███████████████████████████████████████████████| 76/76 [00:06<00:00, 11.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean BLEU Score: 0.8061\n",
      "Mean ROUGE Score: 0.2482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement BLEU evaluation function\n",
    "def compute_bleu(references, candidate):\n",
    "    smoothing = SmoothingFunction().method0\n",
    "    return sentence_bleu(references, candidate, smoothing_function=smoothing)\n",
    "\n",
    "# Implement ROUGE evaluation function\n",
    "def compute_rouge(references, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "    total_score = 0\n",
    "\n",
    "    # Compute ROUGE for each reference\n",
    "    for reference in references:\n",
    "        scores = scorer.score(reference, candidate)\n",
    "        total_score += scores['rouge1'].fmeasure\n",
    "\n",
    "    # Calculate average score\n",
    "    average_score = total_score / len(references)\n",
    "    return average_score\n",
    "\n",
    "# Evaluate BLEU and ROUGE for each query\n",
    "\n",
    "K = 30 # Number of most relevant docs to consider for scoring performance\n",
    "total_bleu_score = 0.0\n",
    "total_rouge_score = 0.0\n",
    "num_queries = 0\n",
    "\n",
    "for query_id, relevant_docs in tqdm(rel_set.items(), desc = 'Computing scores:'):\n",
    "    query_text = queries[int(query_id)]\n",
    "    response = answers_openai[int(query_id)]\n",
    "    # print(query_id, query_text, \"\\n\\nResponse:\\n\", response, \"\\n=========\\nTopmost relevant Doc:\\n\", docs[int(relevant_docs[0])-1]['body'], \"\\n======\\n\")\n",
    "    # print()\n",
    "    \n",
    "    # Evaluate using BLEU\n",
    "    bleu_score = compute_bleu([docs[int(id)]['body'] for id in relevant_docs[:K]], response)\n",
    "    total_bleu_score += bleu_score\n",
    "\n",
    "    # Evaluate using ROUGE\n",
    "    rouge_score = compute_rouge([docs[int(id)]['body'] for id in relevant_docs[:K]], response)\n",
    "    total_rouge_score += rouge_score\n",
    "\n",
    "    num_queries += 1\n",
    "    \n",
    "# Calculate mean scores\n",
    "mean_bleu_score = total_bleu_score / num_queries\n",
    "mean_rouge_score = total_rouge_score / num_queries\n",
    "\n",
    "print(f\"Mean BLEU Score: {mean_bleu_score:.4f}\")\n",
    "print(f\"Mean ROUGE Score: {mean_rouge_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1c385a9-f3ed-41f8-b795-9884885c19a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# da-vinci\n",
    "# Mean BLEU Score: 0.6094\n",
    "# Mean ROUGE Score: 0.1603\n",
    "\n",
    "# gpt-3.5-turbo-instruct\n",
    "# Mean BLEU Score: 0.8061\n",
    "# Mean ROUGE Score: 0.2482"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
