{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c9eb3eb-61a9-4c9c-b1e7-702f33569ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce161597-9abf-4454-8a53-4ad44e7ba862",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf004e8f-ca4b-4463-8f13-d7cdc4432636",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/cisi/CISI.ALL\") as f:\n",
    "    articles = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45fd2fe6-4f16-4aa8-98af-88bd3922436d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.I 1\\n',\n",
       " '.T\\n',\n",
       " '18 Editions of the Dewey Decimal Classifications\\n',\n",
       " '.A\\n',\n",
       " 'Comaromi, J.P.\\n',\n",
       " '.W\\n',\n",
       " '   The present study is a history of the DEWEY Decimal\\n',\n",
       " 'Classification.  The first edition of the DDC was published\\n',\n",
       " 'in 1876, the eighteenth edition in 1971, and future editions\\n',\n",
       " \"will continue to appear as needed.  In spite of the DDC's\\n\",\n",
       " 'long and healthy life, however, its full story has never\\n',\n",
       " 'been told.  There have been biographies of Dewey\\n',\n",
       " 'that briefly describe his system, but this is the first\\n',\n",
       " 'attempt to provide a detailed history of the work that\\n',\n",
       " 'more than any other has spurred the growth of\\n',\n",
       " 'librarianship in this country and abroad.\\n',\n",
       " '.X\\n',\n",
       " '1\\t5\\t1\\n',\n",
       " '92\\t1\\t1\\n',\n",
       " '262\\t1\\t1\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99fe711-33dd-40cb-bed6-2329514318f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_docs(lines):\n",
    "    title_mode = False\n",
    "    body_mode = False\n",
    "    edge_mode = False\n",
    "    title = \"\"\n",
    "    body = \"\"\n",
    "    idx = None\n",
    "    edge_str = \"\"\n",
    "    edges = []\n",
    "    docs = []\n",
    "    for line in lines:\n",
    "        for c in line:\n",
    "            if line.startswith(\".\"):\n",
    "                if line.startswith(\".I\"):\n",
    "                    for e in edge_str.split(\"\\n\"):\n",
    "                        if \"\\t\" in e:\n",
    "                            edges.append((idx, int(e.split(\"\\t\")[0])))\n",
    "                    idx = int(line.split()[1])\n",
    "                    edge_str = \"\"\n",
    "                    edge_mode = False\n",
    "                if line.startswith(\".T\"):\n",
    "                    title_mode = True\n",
    "                    body_mode = False\n",
    "                elif line.startswith(\".W\"):\n",
    "                    title_mode = False\n",
    "                    body_mode = True\n",
    "                elif line.startswith(\".X\"):\n",
    "                    docs.append({\"id\": idx, \"title\": title, \"body\": body})\n",
    "                    title = \"\"\n",
    "                    body = \"\"\n",
    "                    title_mode = False\n",
    "                    body_mode = False\n",
    "                    edge_mode = True\n",
    "                else:\n",
    "                    title_mode = False\n",
    "                    body_mode = False \n",
    "                    edge_mode = False\n",
    "            if title_mode:\n",
    "                title += c\n",
    "            elif body_mode:\n",
    "                body += c\n",
    "            elif edge_mode:\n",
    "                edge_str += c\n",
    "    for e in edge_str.split(\"\\n\"):\n",
    "        if \"\\t\" in e:\n",
    "            edges.append((idx, int(e.split(\"\\t\")[0])))\n",
    "    return [x for x in docs if x[\"title\"]], sorted(list(set(edges)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7b6cd47-3b68-4b30-86fe-63a58cf325dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, edges = extract_docs(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e25e34-9d0a-4e62-9cf7-8b33209b866f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'title': '.T\\n18 Editions of the Dewey Decimal Classifications\\n',\n",
       " 'body': \".W\\n   The present study is a history of the DEWEY Decimal\\nClassification.  The first edition of the DDC was published\\nin 1876, the eighteenth edition in 1971, and future editions\\nwill continue to appear as needed.  In spite of the DDC's\\nlong and healthy life, however, its full story has never\\nbeen told.  There have been biographies of Dewey\\nthat briefly describe his system, but this is the first\\nattempt to provide a detailed history of the work that\\nmore than any other has spurred the growth of\\nlibrarianship in this country and abroad.\\n\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72cfed7e-f8fc-416c-9607-944df38523f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (1, 92), (1, 262), (1, 556), (1, 1004)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "922b6a29-005a-4449-9f44-db6f51b73bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9712d43c-98f2-45e4-b82e-816e8e2a4ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {}\n",
    "\n",
    "idx = None\n",
    "with open(\"../data/cisi/CISI.QRY\") as f:\n",
    "    for query in f.read().split(\".I\"):\n",
    "        for i, line in enumerate(query.split(\"\\n\")):\n",
    "            if not line:\n",
    "                continue\n",
    "            elif i == 0:\n",
    "                idx = int(line)\n",
    "                queries[idx] = \"\"\n",
    "            elif not line.startswith(\".\"):\n",
    "                queries[idx] += \" \"+line\n",
    "        if idx:\n",
    "            queries[idx] = tokenizer(queries[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc111f3e-0515-4d81-92ef-224dc58dfb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dc04a55-82ef-45a0-ae1e-af1c71b79e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = OPENAI_API_KEY)\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    if response and hasattr(response, 'data') and response.data:\n",
    "        embedding = response.data[0].embedding\n",
    "        return embedding\n",
    "    else:\n",
    "        print(\"Invalid response or no embedding data received.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca336b18-cad5-43c4-b02d-56257801f95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings...: 100%|█████████████| 1460/1460 [04:11<00:00,  5.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for docs\n",
    "for doc in tqdm(docs, desc = 'Generating Embeddings'):\n",
    "    combined_text = doc['title'] + \" \" + doc['body']\n",
    "    doc['embedding'] = get_embedding(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101e666-af50-48b6-83a3-49033a63ad24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a1f98a4-0aff-4f5b-8517-bd64ab9c477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|█████████████████████████████████████████| 112/112 [00:43<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for queries\n",
    "for idx, query in tqdm(queries.items(), desc = 'Generating Embeddings'):\n",
    "    query_text = \" \".join(query)\n",
    "    queries[idx] = {'text': query_text, 'embedding': get_embedding(query_text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2017f-bd07-4b29-bb15-1c789ebbf967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2867e1a-8949-439e-9807-75369a5760cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to ./backups/query_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "docs_file_path = './backups/openai_embeddings/doc_embeddings.pkl'\n",
    "query_file_path = './backups/openai_embeddings/query_embeddings.pkl'\n",
    "\n",
    "with open(docs_file_path, 'wb') as file:\n",
    "    pickle.dump(docs, file)\n",
    "\n",
    "print(f\"Embeddings saved to {docs_file_path}\")\n",
    "\n",
    "with open(query_file_path, 'wb') as file:\n",
    "    pickle.dump(queries, file)\n",
    "\n",
    "print(f\"Embeddings saved to {query_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bab8618c-4505-461d-a0fd-57b83cd89b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document embeddings loaded successfully.\n",
      "Query embeddings loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "#### In case of API limit exceeded error, use embeddings from pickle files ###\n",
    "\n",
    "docs_file_path = './backups/openai_embeddings/doc_embeddings.pkl'\n",
    "\n",
    "with open(docs_file_path, 'rb') as file:\n",
    "    loaded_docs = pickle.load(file)\n",
    "\n",
    "print(\"Document embeddings loaded successfully.\")\n",
    "\n",
    "query_file_path = './backups/openai_embeddings/query_embeddings.pkl'\n",
    "\n",
    "# Load the query embeddings from the file\n",
    "with open(query_file_path, 'rb') as file:\n",
    "    loaded_queries = pickle.load(file)\n",
    "    \n",
    "queries = loaded_queries\n",
    "docs = loaded_docs\n",
    "print(\"Query embeddings loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a97908b3-10a9-4671-92a9-8312208e4e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarity scores: 100%|███████████████████████████████████| 112/112 [00:29<00:00,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean execution time for all queries: 261.24 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Calculate cosine similarity for each query-document pair\n",
    "similarity_scores = {}\n",
    "execution_times = [] \n",
    "for query_id, query in tqdm(queries.items(), desc = 'Computing similarity scores'):\n",
    "    query_embedding = query['embedding']\n",
    "    scores = []\n",
    "    start_time = time.time()\n",
    "    for doc in docs:\n",
    "        doc_embedding = doc['embedding']\n",
    "        sim_score = cosine_similarity(query_embedding, doc_embedding)\n",
    "        scores.append((doc['id'], sim_score))\n",
    "    \n",
    "    end_time = time.time()  # Record end time\n",
    "    execution_time = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "    execution_times.append(execution_time)\n",
    "    similarity_scores[query_id] = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "mean_execution_time = sum(execution_times) / len(execution_times)\n",
    "print(f\"Mean execution time for all queries: {mean_execution_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20e79823-58b6-42a1-9e81-9786336308cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f75c3d8-619c-420b-ad45-f24a1d1fecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/cisi/CISI.REL\") as f:\n",
    "    lines = f.read().split('\\n')[:-1]\n",
    "    ground_truth = [[]]*len(lines)\n",
    "    for line in lines:\n",
    "        clean_line = line.strip().replace('\\t',' ').split()\n",
    "        query, doc = [int(num.replace(' ','')) for num in clean_line[:2]]\n",
    "        ground_truth[query].append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14601412-313f-49f8-9c62-3f9d6678c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./backups/ground_truth.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ground_truth, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa174d-4ea7-46f1-bfbe-e2fe85b326d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ground truth from the file\n",
    "with open(\"./backups/ground_truth.pkl\", \"rb\") as f:\n",
    "    ground_truth = pickle.load(f)\n",
    "\n",
    "print(\"Grouth truth loaded succesfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9bfe527-76f1-4aaa-abe8-b5e0b240f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(ranked_docs, relevant_docs, k=10):\n",
    "    retrieved_relevant = 0\n",
    "    for doc_id in ranked_docs[:k]:\n",
    "        if doc_id in relevant_docs:\n",
    "            retrieved_relevant += 1\n",
    "    return retrieved_relevant / k\n",
    "\n",
    "def recall_at_k(ranked_docs, relevant_docs, k=10):\n",
    "    retrieved_relevant = sum(1 for doc_id in ranked_docs[:k] if doc_id in relevant_docs)\n",
    "    return retrieved_relevant / len(relevant_docs) if relevant_docs else 0\n",
    "\n",
    "def dcg_at_k(scores, k=10):\n",
    "    return sum(score / np.log2(idx + 2) for idx, score in enumerate(scores[:k]))\n",
    "\n",
    "def ndcg_at_k(ranked_docs, relevant_docs, k=5):\n",
    "    ideal_scores = [1 if doc_id in relevant_docs else 0 for doc_id in ranked_docs]\n",
    "    actual_scores = [1 if doc_id in relevant_docs else 0 for doc_id in ranked_docs[:k]]\n",
    "    idcg = dcg_at_k(ideal_scores, k)\n",
    "    dcg = dcg_at_k(actual_scores, k)\n",
    "    return dcg / idcg if idcg > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eccde7a-62b9-43eb-b0f1-b877b8da0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [0]*(len(similarity_scores)+1)\n",
    "for idx, scores in similarity_scores.items():\n",
    "    scores_flattened = [doc for doc, score in scores]\n",
    "    predictions[idx] = scores_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1fc5493-8dfd-4ae4-bd6b-bd29b629fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_precision_at_k = np.mean([precision_at_k(preds,label) for preds,label in zip(predictions[1:],ground_truth)])\n",
    "mean_recall_at_k = np.mean([recall_at_k(preds,label) for preds,label in zip(predictions[1:],ground_truth)])\n",
    "mean_ndcg_at_k = np.mean([ndcg_at_k(preds,label) for preds,label in zip(predictions[1:],ground_truth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4010549d-7c63-4df8-814f-6059a4997a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9705357142857144, 0.003116685016974034, 1.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_precision_at_k, mean_recall_at_k, mean_ndcg_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a07e1a-ee8b-4488-b9ce-eaac43319c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
