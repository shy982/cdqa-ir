{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d450d4-a778-4e79-aee4-d061c102d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from operator import itemgetter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4de2cd29-c1f6-4a98-b843-c2fc4961c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Uncomment to use the default DaVinci Model \n",
    "# llm = OpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "# model = 'text-davinci-003'\n",
    "\n",
    "# Uncomment to use the gpt-3.5-turbo-instruct model \n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct', openai_api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "model = 'gpt-3.5-turbo-instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09a1864e-1407-419a-a5a6-9643a32c4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return word_tokenize(text.lower())\n",
    "queries = {}\n",
    "\n",
    "idx = None\n",
    "with open(\"../data/cisi/CISI.QRY\") as f:\n",
    "    for query in f.read().split(\".I\"):\n",
    "        for i, line in enumerate(query.split(\"\\n\")):\n",
    "            if not line:\n",
    "                continue\n",
    "            elif i == 0:\n",
    "                idx = int(line)\n",
    "                queries[idx] = \"\"\n",
    "            elif not line.startswith(\".\"):\n",
    "                queries[idx] += \" \"+line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944e68a-04f8-48f6-b74f-795a5aea06ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53d7d888-6520-4f05-a471-693db3de55ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document embeddings loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "docs_file_path = './backups/openai_embeddings/doc_embeddings.pkl'\n",
    "\n",
    "with open(docs_file_path, 'rb') as file:\n",
    "    docs = pickle.load(file)\n",
    "\n",
    "print(\"Document embeddings loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6126b150-171e-47fb-834f-2e2bcb8720bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".W\\n   The present study is a history of the DEWEY Decimal\\nClassification.  The first edition of the DDC was published\\nin 1876, the eighteenth edition in 1971, and future editions\\nwill continue to appear as needed.  In spite of the DDC's\\nlong and healthy life, however, its full story has never\\nbeen told.  There have been biographies of Dewey\\nthat briefly describe his system, but this is the first\\nattempt to provide a detailed history of the work that\\nmore than any other has spurred the growth of\\nlibrarianship in this country and abroad.\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30166860-696f-4f6d-8a80-80607694bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_set = {}\n",
    "with open(os.path.join(\"../data/cisi/\", 'CISI.REL')) as f:\n",
    "    for l in f.readlines():\n",
    "        qry_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0]\n",
    "        doc_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1]\n",
    "\n",
    "        if qry_id in rel_set:\n",
    "            rel_set[qry_id].append(doc_id)\n",
    "        else:\n",
    "            rel_set[qry_id] = []\n",
    "            rel_set[qry_id].append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f343c5da-8a70-45d1-9b78-24715fdcc70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_reduced_docs_file_path = './backups/documents_lsi.pkl'\n",
    "with open(lsi_reduced_docs_file_path, 'rb') as file:\n",
    "    documents_reduced = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c05190fa-3f5a-40c3-86dd-b609432e49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_reduced_queries_file_path = './backups/queries_lsi.pkl'\n",
    "with open(lsi_reduced_queries_file_path, 'rb') as file:\n",
    "    queries_reduced = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aad306ab-0466-43cd-a3ad-9bc3ba366044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarity scores: 112it [00:00, 232.88it/s]\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Calculate cosine similarity for each query-document pair\n",
    "similarity_scores = {}\n",
    "for query_id, query in tqdm(enumerate(queries_reduced), desc = 'Computing similarity scores'):\n",
    "    scores = []\n",
    "    for doc_id, doc in enumerate(documents_reduced):\n",
    "        sim_score = cosine_similarity(query, doc)\n",
    "        scores.append((doc_id, sim_score))\n",
    "    similarity_scores[query_id] = sorted(scores, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18fe70ae-c732-4d8f-a6d0-25d231e3d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [0]*len(similarity_scores)\n",
    "for idx, scores in similarity_scores.items():\n",
    "    scores_flattened = [doc for doc,score in scores]\n",
    "    predictions[idx] = scores_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a1973-0219-4325-8601-51886ac04f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e00668d4-0650-4dd5-934a-2bca0a29f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load index from file\n",
    "# loaded_faiss_vs = FAISS.load_local(\n",
    "#     folder_path=\"./backups/faiss/\",\n",
    "#     embeddings=OpenAIEmbeddings())\n",
    "\n",
    "# retriever = loaded_faiss_vs.as_retriever(search_kwargs={'k': 10})\n",
    "\n",
    "# Define the RAG pipeline\n",
    "template = \"\"\"\n",
    "Answer the question or Explain the topic given this additional context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f7a5756-298f-425d-be0e-8816c168a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(_docs):\n",
    "    ls = []\n",
    "    for doc in _docs:\n",
    "        if int(doc.page_content) in docs:\n",
    "            ls.append(docs[int(doc.page_content)][\"body\"])\n",
    "    return ls\n",
    "    # return [docs[int(doc.page_content)][\"body\"] for doc in _docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9b67855-db6b-42a7-8c38-20ca81eaaeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ({\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()} \n",
    "         | prompt \n",
    "         | llm \n",
    "         | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31ab46da-3217-41c2-aa0f-6518e6935a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: The use of electronic data processing equipment has been suggested as a potential solution to the mounting arrearages and other processing difficulties faced by research libraries, such as the Library of Congress. Representatives of computer firms have conducted studies indicating that certain areas of library operations could benefit from automation.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mquery_id = 2\n",
    "# print(queries[mquery_id])\n",
    "context_str = '\\n\\n\\n'.join([docs[doc_id]['body'] for doc_id in predictions[mquery_id][:8]])\n",
    "input_data = {\"context\": context_str, \"query\": queries[mquery_id]}\n",
    "chain.invoke(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba9db072-d4e4-4e3a-bff5-735e79954315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking Queries to ChatGPT with RAG: 100%|██████████████████████████████| 76/76 [05:03<00:00,  3.99s/it]\n"
     ]
    }
   ],
   "source": [
    "#### API CALL WARNING #####\n",
    "\n",
    "rag_responses = {}\n",
    "loq = []\n",
    "# Run RAG pipeline for every question\n",
    "for query_id in tqdm(rel_set.keys(), desc = 'Asking Queries to ChatGPT with RAG'):\n",
    "    query_text = queries[int(query_id)]\n",
    "    context_str = '\\n\\n\\n'.join([docs[doc_id]['body'] for doc_id in predictions[int(query_id)]])[:4096]\n",
    "    input_data = {\"context\": context_str, \"query\": queries[int(query_id)]}\n",
    "    response = chain.invoke(input_data)\n",
    "    rag_responses[query_id] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89478ca1-3feb-43e0-83a5-d346aa677657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Searching Biases in Large Interactive Document Retrieval Systems Blair, D.C.     The way that individuals construct and modify search queries on a large interactive document retrieval system is subject to systematic biases similar to those that have been demonstrated in experiments on judgements under uncertainty.  These biases are shared by both naive and sophisticated subjects and cause the inquirer searching for documents on a large interactive system to construct and modify queries inefficiently.  A searching algorithm is suggested that helps the inquirer to avoid the effect of these biases. (JASIS, Vol. 31, No. 4, July 1980, pp. 271-277)\n"
     ]
    }
   ],
   "source": [
    "print(queries[61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb7a72be-5f6b-446e-bb7b-63ad8a8d31db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG responses saved to ./backups/lsi_openai_with_rag_responses_gpt-3.5-turbo-instruct.pkl\n"
     ]
    }
   ],
   "source": [
    "#### DUMP OVERWRITE WARNING ####\n",
    "\n",
    "rag_responses_file_path = './backups/lsi_openai_with_rag_responses_' + model + '.pkl'\n",
    "with open(rag_responses_file_path, 'wb') as file:\n",
    "    pickle.dump(rag_responses, file)\n",
    "\n",
    "print(f\"RAG responses saved to {rag_responses_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69543307-aca1-4247-94a8-1cd6dd983553",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_with_rag_responses_file_path = './backups/lsi_openai_with_rag_responses_' + model + '.pkl'\n",
    "with open(openai_with_rag_responses_file_path, 'rb') as file:\n",
    "    rag_responses = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "694673f9-44bb-47ff-9fc6-1045afff7035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Answer: It can be a challenging task to create descriptive titles for articles, as they must accurately represent the content of the article while also being concise enough to catch the reader's attention. One of the main difficulties in automatically retrieving articles from approximate titles is that the titles may not always accurately reflect the content of the article. This can lead to irrelevant or incorrect results being retrieved. The usual relevance of the content of articles to their titles can vary, as some titles may accurately represent the content while others may only provide a general idea. This is why it is important for titles to be carefully chosen to accurately represent the content of the article.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Sanity\n",
    "rag_responses['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecff5f07-b4d6-4e54-bf87-0d851c3ae049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".W\\n   The present study is a history of the DEWEY Decimal\\nClassification.  The first edition of the DDC was published\\nin 1876, the eighteenth edition in 1971, and future editions\\nwill continue to appear as needed.  In spite of the DDC's\\nlong and healthy life, however, its full story has never\\nbeen told.  There have been biographies of Dewey\\nthat briefly describe his system, but this is the first\\nattempt to provide a detailed history of the work that\\nmore than any other has spurred the growth of\\nlibrarianship in this country and abroad.\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86e89275-062c-49f8-ab9b-ad6bd50f4c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean BLEU Score: 0.7441\n",
      "Mean ROUGE Score: 0.2167\n"
     ]
    }
   ],
   "source": [
    "# Implement BLEU evaluation function\n",
    "def compute_bleu(references, candidate):\n",
    "    smoothing = SmoothingFunction().method0\n",
    "    return sentence_bleu(references, candidate, smoothing_function=smoothing)\n",
    "\n",
    "# Implement ROUGE evaluation function\n",
    "def compute_rouge(references, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "    total_score = 0\n",
    "\n",
    "    # Compute ROUGE for each reference\n",
    "    for reference in references:\n",
    "        scores = scorer.score(reference, candidate)\n",
    "        total_score += scores['rouge1'].fmeasure\n",
    "\n",
    "    # Calculate average score\n",
    "    average_score = total_score / len(references)\n",
    "    return average_score\n",
    "\n",
    "# Evaluate BLEU and ROUGE for each query\n",
    "\n",
    "K = 30 # Number of most relevant docs to consider for scoring performance\n",
    "total_bleu_score = 0.0\n",
    "total_rouge_score = 0.0\n",
    "num_queries = 0\n",
    "\n",
    "for query_id, relevant_docs in rel_set.items():\n",
    "    query_text = queries[int(query_id)]\n",
    "    response = rag_responses[query_id]\n",
    "\n",
    "    # print(query_id, query_text, \"\\n\\nResponse:\\n\", response, \"\\nTopmost relevant Doc:\\n\", docs[int(relevant_docs[0])], \"\\n======\\n\")\n",
    "    \n",
    "    # Evaluate using BLEU\n",
    "    bleu_score = compute_bleu([docs[int(id)]['body'] for id in relevant_docs[:K]], response)\n",
    "    total_bleu_score += bleu_score\n",
    "\n",
    "    # Evaluate using ROUGE\n",
    "    rouge_score = compute_rouge([docs[int(id)]['body'] for id in relevant_docs[:K]], response)\n",
    "    total_rouge_score += rouge_score\n",
    "\n",
    "    num_queries += 1\n",
    "    # if num_queries == 10:\n",
    "    #     break\n",
    "\n",
    "# Calculate mean scores\n",
    "mean_bleu_score = total_bleu_score / num_queries\n",
    "mean_rouge_score = total_rouge_score / num_queries\n",
    "\n",
    "print(f\"Mean BLEU Score: {mean_bleu_score:.4f}\")\n",
    "print(f\"Mean ROUGE Score: {mean_rouge_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22b95aab-2ac2-404f-ba82-55a06e8ae2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# da-vinci\n",
    "# Mean BLEU Score: 0.8224\n",
    "# Mean ROUGE Score: 0.2105\n",
    "\n",
    "# Mean BLEU Score: 0.8377\n",
    "# Mean ROUGE Score: 0.2226\n",
    "\n",
    "# gpt-3.5-turbo-instruct\n",
    "\n",
    "# Mean BLEU Score: 0.7869\n",
    "# Mean ROUGE Score: 0.2407\n",
    "\n",
    "# Mean BLEU Score: 0.7937\n",
    "# Mean ROUGE Score: 0.2425\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
