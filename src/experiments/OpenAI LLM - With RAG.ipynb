{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53d450d4-a778-4e79-aee4-d061c102d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from operator import itemgetter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4de2cd29-c1f6-4a98-b843-c2fc4961c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = OpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09a1864e-1407-419a-a5a6-9643a32c4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return word_tokenize(text.lower())\n",
    "queries = {}\n",
    "\n",
    "idx = None\n",
    "with open(\"../data/cisi/CISI.QRY\") as f:\n",
    "    for query in f.read().split(\".I\"):\n",
    "        for i, line in enumerate(query.split(\"\\n\")):\n",
    "            if not line:\n",
    "                continue\n",
    "            elif i == 0:\n",
    "                idx = int(line)\n",
    "                queries[idx] = \"\"\n",
    "            elif not line.startswith(\".\"):\n",
    "                queries[idx] += \" \"+line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8944e68a-04f8-48f6-b74f-795a5aea06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53d7d888-6520-4f05-a471-693db3de55ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document embeddings loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "docs_file_path = './backups/openai_embeddings/doc_embeddings.pkl'\n",
    "\n",
    "with open(docs_file_path, 'rb') as file:\n",
    "    docs = pickle.load(file)\n",
    "\n",
    "print(\"Document embeddings loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6126b150-171e-47fb-834f-2e2bcb8720bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.W\\nThis report is an analysis of 6300 acts of use\\nin 104 technical libraries in the United Kingdom.\\nLibrary use is only one aspect of the wider pattern of\\ninformation use.  Information transfer in libraries is\\nrestricted to the use of documents.  It takes no\\naccount of documents used outside the library, still\\nless of information transferred orally from person\\nto person.  The library acts as a channel in only a\\nproportion of the situations in which information is\\ntransferred.\\nTaking technical information transfer as a whole,\\nthere is no doubt that this proportion is not the\\nmajor one.  There are users of technical information -\\nparticularly in technology rather than science -\\nwho visit libraries rarely if at all, relying on desk\\ncollections of handbooks, current periodicals and personal\\ncontact with their colleagues and with people in other\\norganizations.  Even regular library users also receive\\ninformation in other ways.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30166860-696f-4f6d-8a80-80607694bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_set = {}\n",
    "with open(os.path.join(\"../data/cisi/\", 'CISI.REL')) as f:\n",
    "    for l in f.readlines():\n",
    "        qry_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0]\n",
    "        doc_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1]\n",
    "\n",
    "        if qry_id in rel_set:\n",
    "            rel_set[qry_id].append(doc_id)\n",
    "        else:\n",
    "            rel_set[qry_id] = []\n",
    "            rel_set[qry_id].append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "709a2a83-d736-4448-9584-905b2db58900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DUMP OVERWRITE WARNING ####\n",
    "\n",
    "# Build the document vector store\n",
    "\n",
    "K = 10 # Hyperparameter governing how many top ranking documents are used for RAG IR\n",
    "doc_texts = [docs[int(doc_id) - 1]['body'] for query_id in rel_set.keys() for doc_id in rel_set[query_id][:K]]\n",
    "\n",
    "openai_answers_file_path = './backups/rag_contexts/cisi/doc_texts.pkl'\n",
    "\n",
    "with open(openai_answers_file_path, 'wb') as file:\n",
    "    pickle.dump(doc_texts, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "add8bf19-620e-47d9-9e41-45716c19fee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document texts loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "docs_file_path = './backups/rag_contexts/cisi/doc_texts.pkl'\n",
    "\n",
    "with open(docs_file_path, 'rb') as file:\n",
    "    doc_texts = pickle.load(file)\n",
    "\n",
    "print(\"Document texts loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "488a20a0-ee6c-4234-8ca1-a8f72d7d8864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the document vector store\n",
    "# doc_texts = [docs[int(doc_id) - 1]['body'] for query_id in rel_set.keys() for doc_id in rel_set[query_id]]\n",
    "vectorstore = FAISS.from_texts(doc_texts, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Define the RAG pipeline\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "              {context}\n",
    "              Question: {question}\n",
    "            \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fb79e70-9345-4fe9-bb82-a96dffa3ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code converts the question and context into a prompt, passes it to the llm, and parses the llm output \n",
    "chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt  | llm | StrOutputParser() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba9db072-d4e4-4e3a-bff5-735e79954315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG responses saved to ./backups/openai_with_rag_responses.pkl\n"
     ]
    }
   ],
   "source": [
    "#### API CALL WARNING #####\n",
    "\n",
    "rag_responses = {}\n",
    "\n",
    "# Run RAG pipeline for every question\n",
    "for query_id in tqdm(rel_set.keys(), desc = 'Asking Queries to ChatGPT with RAG'):\n",
    "    query_text = queries[int(query_id)]\n",
    "    response = chain.invoke(query_text)\n",
    "    rag_responses[query_id] = response\n",
    "\n",
    "# Save RAG responses to a pickle file\n",
    "rag_responses_file_path = './backups/openai_with_rag_responses.pkl'\n",
    "with open(rag_responses_file_path, 'wb') as file:\n",
    "    pickle.dump(rag_responses, file)\n",
    "\n",
    "print(f\"RAG responses saved to {rag_responses_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69543307-aca1-4247-94a8-1cd6dd983553",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_with_rag_responses_file_path = './backups/openai_with_rag_responses.pkl'\n",
    "with open(openai_with_rag_responses_file_path, 'rb') as file:\n",
    "    rag_responses = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "694673f9-44bb-47ff-9fc6-1045afff7035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: Information dissemination by journals and periodicals is an important part of scientific libraries, as it provides access to a wealth of experimental data. In order to make the most efficient use of funds, libraries must adopt a highly selective purchasing plan to ensure that the needs of readers are met. However, the development of a national information system is beginning to encroach on the domain of the primary publication system, which may endanger orderly communication through research journals. Additionally, increased distribution of unedited, unreferred and unproofed preprints could further disrupt journals or transform them into depositories.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Sanity\n",
    "rag_responses['30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e89275-062c-49f8-ab9b-ad6bd50f4c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
