{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0514178-7e33-4887-afcd-ad0cd37d5662",
   "metadata": {},
   "source": [
    "## The OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba0c1ef-06cb-45cb-90e4-efda7ea7deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\n",
    "# https://platform.openai.com/docs/api-reference/authentication\n",
    "import os\n",
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "openai.organization = \"org-mGi0K4AmKu41HxKowsVRjoNh\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c1edf4-5a64-4852-be59-4146b8af38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81385b84-f497-48d2-9c7c-444ff7bd5d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe world’s funniest joke is a subjective matter and there has not been any scientific analysis conducted on the topic. What one person may find funny, another may find dull or even offensive.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test our connection to openAI\n",
    "llm.invoke(\"What's the world's funniest joke? Has there ever been any scientific analysis?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad6c18-eafe-4528-87ae-366fc2752611",
   "metadata": {},
   "source": [
    "## LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e91e6b-7b5a-4585-b13c-275dbfdb4c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLangChain Conversational agents are artificial intelligence-based conversational agents that are designed to simulate human conversation. They are powered by natural language processing and machine learning technologies, enabling them to understand and respond to user input. They can be used for a range of applications, from customer service and support to providing personalized recommendations.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What are the LangChain Conversational agents?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c382f0-f533-44e0-884c-5b545af8617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "chat_model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e5cdc629-f85b-4b20-94dd-4b515aacd1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nPaceLife Technologies'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes pacemakers?\"\n",
    "llm.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "bb2970e7-8ff6-4010-8964-5af80ae6bae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HeartSync Technologies'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cd9889-b832-4f01-b8ff-9b4445a79bed",
   "metadata": {},
   "source": [
    "### PromptTemplate + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "aa0b3949-d7ed-4e4b-9cd8-8d111bcc4cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nA41.0 - Septic shock with acute organ dysfunction\\nA41.1 - Septic shock with acute renal failure\\nA41.2 - Septic shock with acute respiratory failure\\nA41.3 - Septic shock with acute cardiorespiratory failure\\nA41.4 - Septic shock with multiple organ dysfunction syndrome\\nA41.9 - Septic shock, unspecified'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"List all ICD10 code for {foo}.\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"foo\": \"septic shock\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f943a40-0b6f-4010-a444-3fc7e90a7ac3",
   "metadata": {},
   "source": [
    "### Retrieval augmented generation (RAG) with Longchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db4bd3e7-8b04-47fd-9e30-868f02c95ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "897de89c-3c8b-49a7-8b3a-d921f0d4cbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nFAISS (Facebook Artificial Intelligence Similarity Search) is an open source library for efficient similarity search and clustering of high-dimensional data. It was developed by Facebook AI Research and is written in C++ with complete wrappers for Python. FAISS allows for the efficient storage and retrieval of vectors in large datasets, making it useful for tasks such as large-scale information retrieval, recommendation systems, document search, and clustering.'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is the FAISS library?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "841ed4b9-06bf-4602-86d8-aaf503b46490",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_note = \"A 28-year-old previously healthy adult patient presented with tachycardia, fever, and mental confusion. The symptoms started after a cut to his leg while gardening.\"\n",
    "vectorstore = FAISS.from_texts([clinical_note], \n",
    "                               embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever() # this implements a RAG\n",
    "\n",
    "template =  \"\"\"Answer the question based only on the following context:\n",
    "            {context}\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "710b13ed-aa36-4924-bb0d-f6c20caf8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code converts the question and context into a prompt, passes it to the llm, and parses the llm output \n",
    "chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt  | llm | StrOutputParser() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1f50a939-3b8e-40d1-aee8-64a2f037d9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: The patient got his cut while gardening.'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"When did this patient get his cut?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a525be95-575c-4955-a07c-a8180bf58671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAnswer: The cut was to the patient's leg.\""
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the location of the cut?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "74ee8244-d0be-4a5f-896a-9a92436c2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try to see if we can get the answers in another language!\n",
    "\n",
    "template =  \"\"\"Answer the question based only on the following context:\n",
    "            {context}\n",
    "            Question: {question}\n",
    "            Answer in the following language: {language}\n",
    "            \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = { \"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\"), \"language\": itemgetter(\"language\")} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a61fe2b2-8707-40fd-be19-7bf7622d46da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEl paciente recibió su corte después de jardinería.'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"When did this patient get his cut?\", \"language\": \"spanish\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b3e74c39-4972-4c1a-9dd0-ae58832d9e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEl corte está en la pierna.'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is the location of the cut?\", \"language\": \"spanish\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502edbe1-815a-4183-ba0d-7f22e00865cc",
   "metadata": {},
   "source": [
    "### RAG with API calls: A Wikipedia Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "51f3267d-ca0c-49fe-ae5c-1a128356120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikipediaapi import Wikipedia\n",
    "wiki = Wikipedia('ShamimBot/0.0', 'en')\n",
    "jh_page = wiki.page('Social_determinants_of_health').text\n",
    "jh_page = jh_page.split('\\nReferences\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4535cbaf-1069-43b7-9a6a-7604fff2cd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  social determinants of health (SDOH) are the economic and social conditions that influence individual and group differences in health status. They are the health promoting factors found in one's living and working conditions (such as the distribution of income, wealth, influence, and power), rather than individual risk factors (such as behavioral risk factors or genetics) that influence the risk for a disease, or vulnerability to disease or injury. The distributions of social determinants a\n"
     ]
    }
   ],
   "source": [
    "print(jh_page[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "81e58049-77d8-4ea1-8f55-c6cde50e9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now going to store the wikipedia page in our vector store and use it as needed to answer questions\n",
    "vectorstore = FAISS.from_texts([jh_page[:500]],  embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "              {context}\n",
    "              Question: {question}\n",
    "            \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "2cd3566e-6500-4b17-8746-cfe45edf86fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt  | llm | StrOutputParser() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "beb7ea82-baf6-4d74-8943-f665fb70622e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: Health promoting factors can include the distribution of income, wealth, influence, and power.'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What are some health promoting factors?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45968e4a-d7ed-437d-8024-c2d84eb4924b",
   "metadata": {},
   "source": [
    "### Running Arbitrary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c60c4cd0-6a5b-49e7-a557-c0a72f09694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "chain = {\"a\": itemgetter(\"arg1\") | RunnableLambda(length_function), \"b\": itemgetter(\"arg2\") | RunnableLambda(length_function) } | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5a3d27a3-854e-4773-a37a-c8159e11ae94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nRobot: 10'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"arg1\": \"patient\",\"arg2\":\"abx\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9f47e-676f-43c7-a478-0487c20415e6",
   "metadata": {},
   "source": [
    "### Agents and Tools!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d2d218f8-b75f-4ebf-8f45-492423f8d48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To calculate the total amount of fluids your patient will receive, you need to multiply the infusion rate (30 mL/kg/hour) by the patient's weight (85 kg) and the duration of the infusion (3 hours).\\n\\nTotal amount of fluids = Infusion rate × Weight × Duration\\n\\nTotal amount of fluids = 30 mL/kg/hour × 85 kg × 3 hours\\n\\nTotal amount of fluids = 7650 mL\\n\\nTherefore, your patient will receive 7650 mL (or 7.65 liters) of fluids during the 3-hour IV infusion.\""
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "tools = load_tools( [\"llm-math\"], llm=model )\n",
    "\n",
    "template =  \"\"\"You're an ED doctor. Your patient is 85kg.\n",
    "              Question: {question}\n",
    "            \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = ( {\"question\": RunnablePassthrough()}  | prompt | llm  | StrOutputParser())\n",
    "chain.invoke(\"How many liters of fluids my patient will receive if administered 30 mL/kg/hour IV infusion over 3 hours?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35afcc3f-5dba-4ec7-b4dc-4c502b3e92b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if patient weight is unknown?\n",
    "model.invoke(\"How many liters of fluids a patient will receive if administered 30 mL/kg/hour IV infusion over 3 hours?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e8a75f3b-a1b7-4bf8-a159-fa1bef509ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To calculate the total amount of fluid your patient will receive, we need to know their weight. Please provide the patient's weight in kilograms.\""
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"You're an ED doctor.\n",
    "              {context}\n",
    "              Question: {question}\n",
    "            \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()}  | prompt | llm  | StrOutputParser())\n",
    "chain.invoke(\"How many liters of fluids my patient will receive if administered 30 mL/kg/hour IV infusion over 3 hours?\") # What is the patient weight? RAG to come to rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6bb6a6-67dd-4963-ba00-e2aa0cfa785b",
   "metadata": {},
   "source": [
    "#### Let's now build an agent that can use tools!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "983d9226-715b-4b0c-80f3-ee32f208513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "996b83cf-26ec-4288-91a5-eb572e90557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts([clinical_note,\"Temperature=99, Pulse=110, BP=100/60, Respiration=24, SpO2=92%\", 'body weight = 85 kg', \"redness, warmth, tenderness, and swelling of the skin\"], \n",
    "                               embedding=OpenAIEmbeddings())\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever = vectorstore.as_retriever()\n",
    "tools = load_tools(\n",
    "    [\"llm-math\"],\n",
    "    llm = llm \n",
    ")\n",
    "\n",
    "tools.append(\n",
    " create_retriever_tool(\n",
    "    retriever,      #--> here is a RAG tool \n",
    "    \"search_patient_record\",\n",
    "    \"Searches and returns patient weight and physicals.\" # note the description of this tool. The agent will use this description to decide which tool to use!\n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e2a17a37-006c-440c-a6e4-a6e212f65ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-mGi0K4AmKu41HxKowsVRjoNh on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-mGi0K4AmKu41HxKowsVRjoNh on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-mGi0K4AmKu41HxKowsVRjoNh on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-mGi0K4AmKu41HxKowsVRjoNh on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_patient_record` with `patient_id`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[Document(page_content='A 28-year-old previously healthy adult patient presented with tachycardia, fever, and mental confusion. The symptoms started after a cut to his leg while gardening.', metadata={}), Document(page_content='body weight = 85 kg', metadata={}), Document(page_content='Temperature=99, Pulse=110, BP=100/60, Respiration=24, SpO2=92%', metadata={}), Document(page_content='redness, warmth, tenderness, and swelling of the skin', metadata={})]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Calculator` with `30 * 85 * 3`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-mGi0K4AmKu41HxKowsVRjoNh on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-mGi0K4AmKu41HxKowsVRjoNh on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-mGi0K4AmKu41HxKowsVRjoNh on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-mGi0K4AmKu41HxKowsVRjoNh on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-mGi0K4AmKu41HxKowsVRjoNh on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mAnswer: 7650\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-mGi0K4AmKu41HxKowsVRjoNh on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-mGi0K4AmKu41HxKowsVRjoNh on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mYour patient will receive 7650 mL of fluids if administered 30 mL/kg/hour IV infusion over 3 hours. To convert this to liters, divide by 1000:\n",
      "\n",
      "7650 mL = 7.65 liters\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent_executor = create_conversational_retrieval_agent(llm, tools, verbose=True)\n",
    "result = agent_executor({\"input\": \"How many liters of fluids my patient will receive if administered 30 mL/kg/hour IV infusion over 3 hours?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
