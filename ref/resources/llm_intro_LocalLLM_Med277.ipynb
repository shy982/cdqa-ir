{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417575b7-a036-4d49-8720-89eb37cef486",
   "metadata": {},
   "source": [
    "## Use a Local LLM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f228c0e-4ed2-4645-bdd6-1e623b55739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama2 7B parameter model\n",
    "# wget https://huggingface.co/localmodels/Llama-2-7B-Chat-ggml/resolve/main/llama-2-7b-chat.ggmlv3.q4_K_S.bin\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7bdc2fe-7704-4838-b36d-bf05f4e22917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/snemati/Documents/Git_Repo/h2ogpt/llama-2-7b-chat.ggmlv3.q4_K_S.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 14 (mostly Q4_K - Small)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 3647.96 MB (+ 1024.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =  153.35 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(model_path=\"/Users/snemati/Documents/Git_Repo/h2ogpt/llama-2-7b-chat.ggmlv3.q4_K_S.bin\", n_ctx=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97067454-1f72-457a-891a-228281e77f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  9313.78 ms\n",
      "llama_print_timings:      sample time =   382.23 ms /   220 runs   (    1.74 ms per token,   575.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 46892.12 ms /   220 runs   (  213.15 ms per token,     4.69 tokens per second)\n",
      "llama_print_timings:       total time = 48065.69 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nLlama.cpp is a C++ library for building and using linters, specifically the Llama language. It provides an easy-to-use API for defining and executing linters, as well as some utilities for working with linting rules.\\n\\nThe main components of Llama.cpp are:\\n\\n1. A set of standard linters that can be used to check code against a set of rules. These include checks for things like coding style, naming conventions, and best practices.\\n2. A mechanism for defining custom linting rules. This allows developers to create their own checks and enforce their own coding standards.\\n3. Utilities for working with linting rules, such as parsing and validating them.\\n\\nLlama.cpp is designed to be easy to use and flexible, allowing developers to easily integrate it into their development workflows. It can be used in combination with other tools, such as linters and code editors, to provide a complete linting solution.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What is Llama.cpp?\") # What do you think about the quality of this response? We will get back this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56c050a4-902b-4700-8f71-b84e6a6547bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  9313.78 ms\n",
      "llama_print_timings:      sample time =   205.79 ms /   122 runs   (    1.69 ms per token,   592.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 23308.80 ms /   122 runs   (  191.06 ms per token,     5.23 tokens per second)\n",
      "llama_print_timings:       total time = 23934.24 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n Why is it important for healthcare providers to understand and address Social Determinants of Health (SDoH)?\\n What are the key drivers of social determinants of health?\\n How can healthcare providers use data to better understand SDoH and improve patient outcomes?\\n What are some potential challenges or limitations of using data to address SDoH?\\nWhat is the role of policy makers in addressing SDoH through legislation or regulatory actions?\\nHow can community-based organizations play a critical role in improving social determinants of health?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What is SDoH?\") # What do you think about the quality of this response? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16031e-3ff0-4287-b009-34f1eacd2ea1",
   "metadata": {},
   "source": [
    "### Let's perform named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ba310d5d-299b-48dd-b8a5-0e2cbb6dd146",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "              Detect named entities in following text delimited by triple backquotes.\n",
    "              Return your response in json format \"named entity\",\"type\".\n",
    "              Return all entities\n",
    "              ```{text}```\n",
    "              json format file:\n",
    "           \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "299c9f72-325b-4f43-a865-10020c28f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "                \"named entity\", \"type\"\n",
      "                \"patient\", \"person\"\n",
      "                \"symptoms\", \"medical_sign\"\n",
      "                \"cut\", \"medical_procedure\"\n",
      "            }\n",
      "\n",
      "        Answer:\n",
      "        Sure, I can help you with that! Here's the response from the trained model in JSON format:\n",
      "        {\n",
      "            \"named entity\": [\n",
      "                \"patient\",\n",
      "                \"symptoms\",\n",
      "                \"cut\"\n",
      "            ],\n",
      "            \"type\": [\n",
      "                \"person\",\n",
      "                \"medical_sign\",\n",
      "                \"medical_procedure\"\n",
      "            ]\n",
      "        }\n",
      "\n",
      "Explanation:\n",
      "The trained model has detected the following named entities in the given text:\n",
      "\n",
      "* \"patient\": A person who is the subject of the medical condition. (Type: \"person\")\n",
      "* \"symptoms\": The medical signs or conditions exhibited by a patient. (Type: \"medical_sign\")\n",
      "* \"cut\": A medical procedure that involves cutting or removing a part of the body. (Type: \"medical_procedure\")\n",
      "\n",
      "These named entities are then mapped to their corresponding types,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  7200.78 ms\n",
      "llama_print_timings:      sample time =   276.03 ms /   256 runs   (    1.08 ms per token,   927.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8366.51 ms /    92 tokens (   90.94 ms per token,    11.00 tokens per second)\n",
      "llama_print_timings:        eval time = 33991.92 ms /   255 runs   (  133.30 ms per token,     7.50 tokens per second)\n",
      "llama_print_timings:       total time = 43292.54 ms\n"
     ]
    }
   ],
   "source": [
    "clinical_note = \"\"\"A 28-year-old previously healthy adult patient presented with tachycardia, fever, and mental confusion. The symptoms started after a cut to his leg while gardening.\"\"\"\n",
    "answer = llm_chain.run(clinical_note)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cff9ae-1a06-476b-b388-a57731fe82fe",
   "metadata": {},
   "source": [
    "### Okay, now let's look at an example of differential diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "08f28418-39bf-4a98-a5c0-5f902339ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "              You are an ER doctor.\n",
    "              Return the top three differentials in following text delimited by triple backquotes.\n",
    "              ```{text}```\n",
    "              Stay concise.\n",
    "           \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6f48c527-f4b6-4876-b9ea-dc0ade928511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```\n",
      "            1. Bacterial sepsis\n",
      "            2. Meningitis\n",
      "            3. Encephalitis\n",
      "            ```\n",
      "     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  7200.78 ms\n",
      "llama_print_timings:      sample time =    35.11 ms /    32 runs   (    1.10 ms per token,   911.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7469.38 ms /    80 tokens (   93.37 ms per token,    10.71 tokens per second)\n",
      "llama_print_timings:        eval time =  3857.90 ms /    32 runs   (  120.56 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time = 11454.35 ms\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.run(clinical_note))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376efcdd-ec80-412d-8886-94ff7e26002a",
   "metadata": {},
   "source": [
    "### Now let's try RAG with our local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5534f433-ca00-47a1-91bb-d7d995dfcb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/snemati/Documents/Git_Repo/h2ogpt/llama-2-7b-chat.ggmlv3.q4_K_S.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 14 (mostly Q4_K - Small)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 3647.96 MB (+  512.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  512.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   71.84 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "\n",
      "llama_print_timings:        load time =  4702.62 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  9685.62 ms /    43 tokens (  225.25 ms per token,     4.44 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  9714.96 ms\n"
     ]
    }
   ],
   "source": [
    "## Extracting and organizing history of present illness into predetermined categories\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "# Make sure the model path is correct for your system!\n",
    "llama = LlamaCppEmbeddings(model_path=\"/Users/snemati/Documents/Git_Repo/h2ogpt/llama-2-7b-chat.ggmlv3.q4_K_S.bin\")\n",
    "\n",
    "vectorstore = FAISS.from_texts([clinical_note], embedding=llama)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the Question by addressing the following 10 categories:\n",
    "              1. General\n",
    "              2. Skin\n",
    "              3. HEENT\n",
    "              4. Pulmonary\n",
    "              5. Cardiovascular\n",
    "              6. Gastrointestinal\n",
    "              7. Genitourinary\n",
    "              8. Musculoskeletal\n",
    "              9. Neurologic\n",
    "              10. Psychiatric\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "66ae6797-1613-467d-9c72-3554aa74fa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  4702.62 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  1405.86 ms /    10 tokens (  140.59 ms per token,     7.11 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  1412.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  7200.78 ms\n",
      "llama_print_timings:      sample time =   402.99 ms /   256 runs   (    1.57 ms per token,   635.26 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19221.57 ms /   147 tokens (  130.76 ms per token,     7.65 tokens per second)\n",
      "llama_print_timings:        eval time = 65259.17 ms /   255 runs   (  255.92 ms per token,     3.91 tokens per second)\n",
      "llama_print_timings:       total time = 86322.09 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAnswer:\\n\\n1. General: The patient presents with tachycardia (rapid heart rate), fever (elevated body temperature), and mental confusion, which are indicative of a systemic inflammatory response. These symptoms suggest that the patient's body is responding to an infection or injury, rather than a localized cut to the leg.\\n2. Skin: There is no mention of any skin changes or lesions related to the cut or the infection.\\n3. HEENT: There is no information about any changes in the patient's breathing, nasal discharge, or eyes.\\n4. Pulmonary: The patient's symptoms do not suggest any pulmonary involvement, such as cough or shortness of breath.\\n5. Cardiovascular: The patient's tachycardia (rapid heart rate) is a cardiovascular symptom that may indicate an infection or inflammation affecting the cardiovascular system.\\n6. Gastrointestinal: There is no information about any gastrointestinal symptoms, such as abdominal pain or diarrhea\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt  | model | StrOutputParser() )\n",
    "chain.invoke(\"What is the history of present illness?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00b128-eb29-4434-91ea-ffd2cbb931f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b787cf2b-ed0f-449e-93e2-7fe5702a52af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  4702.62 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  5746.72 ms /    43 tokens (  133.64 ms per token,     7.48 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  5763.96 ms\n",
      "\n",
      "llama_print_timings:        load time =  4702.62 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 15309.41 ms /    39 tokens (  392.55 ms per token,     2.55 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 15335.04 ms\n",
      "\n",
      "llama_print_timings:        load time =  4702.62 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time = 10186.51 ms /    16 tokens (  636.66 ms per token,     1.57 tokens per second)\n",
      "llama_print_timings:        eval time =   462.50 ms /     1 runs   (  462.50 ms per token,     2.16 tokens per second)\n",
      "llama_print_timings:       total time = 10668.33 ms\n"
     ]
    }
   ],
   "source": [
    "# How about a physical exam with some vitals and impressions?\n",
    "vectorstore = FAISS.from_texts([clinical_note,\"Temperature=99, Pulse=110, BP=100/60, Respiration=24, SpO2=92%\", 'body weight = 85 kg', \"redness, warmth, tenderness, and swelling of the skin\"], \n",
    "                               embedding=llama)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the Question by addressing the following 11 categories:\n",
    "            1. Vital signs\n",
    "            \t- Temperature\n",
    "            \t- Pulse\n",
    "            \t- BP\n",
    "            \t- Respirations \n",
    "            \t- SpO2\n",
    "            2. General appearance\n",
    "            3. Skin\n",
    "            4. HEENT\n",
    "            5. Pulmonary\n",
    "            6. Cardiovascular\n",
    "            7. Gastrointestinal\n",
    "            8. Genitourinary\n",
    "            9. Musculoskeletal\n",
    "            10. Neurologic\n",
    "            11. Mental status\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8df091db-979d-49af-bc88-d63c6520caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  4702.62 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =  3068.83 ms /     8 tokens (  383.60 ms per token,     2.61 tokens per second)\n",
      "llama_print_timings:        eval time =   359.79 ms /     1 runs   (  359.79 ms per token,     2.78 tokens per second)\n",
      "llama_print_timings:       total time =  3441.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  7200.78 ms\n",
      "llama_print_timings:      sample time =   505.11 ms /   256 runs   (    1.97 ms per token,   506.82 tokens per second)\n",
      "llama_print_timings: prompt eval time = 80939.63 ms /   261 tokens (  310.11 ms per token,     3.22 tokens per second)\n",
      "llama_print_timings:        eval time = 74158.55 ms /   255 runs   (  290.82 ms per token,     3.44 tokens per second)\n",
      "llama_print_timings:       total time = 157027.58 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSolution: Based on the given information, the physical examination results are as follows:\\n\\n1. Vital signs:\\n\\t* Temperature: 99°F (37.2°C)\\n\\t* Pulse: 110 beats per minute (bpm)\\n\\t* Blood pressure (BP): 100/60 mmHg\\n\\t* Respirations: 24 breaths per minute (bpm)\\n\\t* SpO2: 92%\\n2. General appearance: There is redness, warmth, tenderness, and swelling of the skin.\\n3. Skin: The skin appears red, warm, and tender to the touch.\\n4. HEENT: None specified in the given information.\\n5. Pulmonary: None specified in the given information.\\n6. Cardiovascular: Tachycardia (fast heart rate) is present.\\n7. Gastrointestinal: None specified in the given information.\\n8. Genitourinary: None specified in the given information.\\n9. Musculoskeletal: None specified in the given information.\\n'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt  | model | StrOutputParser() )\n",
    "chain.invoke(\"What is the physical examination results?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1778cf2-2a64-4914-bdf6-cc4851df810a",
   "metadata": {},
   "source": [
    "## Let's try this with ChatGPT!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec458be-3b5e-4042-8c2a-8a7e02966a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import ChatCompletion,Completion\n",
    "import os\n",
    "openai.organization = \"org-mGi0K4AmKu41HxKowsVRjoNh\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "da35be57-2a87-4e7a-9714-4138c578d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical exam with some vitals and impressions\n",
    "llm = OpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "vectorstore = FAISS.from_texts([clinical_note,\"Temperature=99, Pulse=110, BP=100/60, Respiration=24, SpO2=92%\", \"redness, warmth, tenderness, and swelling of the skin\"], \n",
    "                               embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the Question by addressing the following 11 categories:\n",
    "            1. Vital signs\n",
    "            \t- Temperature\n",
    "            \t- Pulse\n",
    "            \t- BP\n",
    "            \t- Respirations \n",
    "            \t- SpO2\n",
    "            2. General appearance\n",
    "            3. Skin\n",
    "            4. HEENT\n",
    "            5. Pulmonary\n",
    "            6. Cardiovascular\n",
    "            7. Gastrointestinal\n",
    "            8. Genitourinary\n",
    "            9. Musculoskeletal\n",
    "            10. Neurologic\n",
    "            11. Mental status\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e61bbb53-aa07-445e-8e9a-d91818af0713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided information, the physical examination results are as follows:\\n\\n1. Vital signs:\\n   - Temperature: 99°F\\n   - Pulse: 110 beats per minute\\n   - Blood Pressure: 100/60 mmHg\\n   - Respirations: 24 breaths per minute\\n   - SpO2: 92%\\n\\n2. General appearance: Not mentioned in the provided information.\\n\\n3. Skin: The patient has redness, warmth, tenderness, and swelling of the skin, which may indicate inflammation or infection.\\n\\n4. HEENT (Head, Eyes, Ears, Nose, and Throat): Not mentioned in the provided information.\\n\\n5. Pulmonary: Not mentioned in the provided information, but the patient's SpO2 level of 92% suggests a potential issue with oxygenation.\\n\\n6. Cardiovascular: Not mentioned in the provided information, except for tachycardia (elevated heart rate) of 110 beats per minute.\\n\\n7. Gastrointestinal: Not mentioned in the provided information.\\n\\n8. Genitourinary: Not mentioned in the provided information.\\n\\n9. Musculoskeletal: The patient has a cut to his leg while gardening, which may suggest a musculoskeletal injury.\\n\\n10. Neurologic: The patient presents with mental confusion, which may indicate a neurological issue.\\n\\n11. Mental status: The patient is experiencing mental confusion, as mentioned in the provided information.\\n\\nPlease note that the information provided is limited, and a comprehensive physical examination would require a more detailed assessment in each category.\""
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt  | model | StrOutputParser() )\n",
    "chain.invoke(\"What is the physical examination results?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5414df1e-3c26-4f97-847e-a60a79e2a5b0",
   "metadata": {},
   "source": [
    "### The H2OGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb68e11-d692-4f2c-bae0-fe49c3905e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/h2oai/h2ogpt/blob/main/docs/README_MACOS.md --> h2oGPT personal LLM\n",
    "# cd /Users/snemati/Documents/Git_Repo/h2ogpt\n",
    "# conda activate h2ogpt\n",
    "# python generate.py --base_model='llama' --prompt_type=llama2 --score_model=None --langchain_mode='UserData' --user_path=user_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f33020-a614-46d9-a96d-8630f0ce5476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
